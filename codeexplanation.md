This Python code implements a neural network to solve the XOR problem. The neural network consists of an input layer, a hidden layer with ReLU activation, and an output layer with sigmoid activation. Furthermore, it utilizes cross-validation, mini-batch Stochastic Gradient Descent (SGD), and visualization of decision boundaries. In summary, the code begins by loading and preprocessing the XOR dataset, then it initializes weights and biases, then it uses cross validation to train and evaluate the model on different folds and then it trains the model on the entire training set. After, it assesses the models performance on the training and test sets by using classification metrics. Lastly, it plots the decision boundary to confirm the model's ability to classify the XOR dataset.


Results of the code:
The cross-validation results demonstrate an accuracy of 97.63% with a standard deviation of 1.21%. This means that the model has strong generalization during cross-validation due to high accuracy and low variability across folds. For the training results, the loss starts around 0.68 and steadily decreases until reaching 0.0902 by 100 epochs. This shows consistent convergence without instability. Performance metrics on the training set have a precision of 0.98–0.99 (for both classes), a recall of 0.98–0.99 (this is a bit lower for class 1), and a balanced F1-score of 0.99 (for both classes), and an overall accuracy of 99%. On the test set, the performance metrics are similarly strong, with precision, recall, and F1-scores ranging from 0.99–1.00, and an overall accuracy of 99%. This shows that the test set does a very good job on generalization to unseen data.